import streamlit as st
import streamlit.components.v1 as html
from protlearn.features import (
    paac,
    aac,
    aaindex1,
    entropy,
    atc,
    ctd,
    moran,
    geary,
    qso,
    ngram,
    motif,
    cksaap,
)
import pandas as pd
import numpy as np
import base64
from PIL import Image
from io import BytesIO

# from icecream import ic

# sklean

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import (
    RandomForestClassifier,
    AdaBoostClassifier,
    GradientBoostingClassifier,
)
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
import joblib
import json  
import os  
 #åŠ è½½æ¨¡å‹ 
def load_rf_model():  
    model_path = "rf_model.pkl"  
    model = joblib.load(model_path)  
    return model
def load_gb_model():  
    model_path = "gb_model.pkl"  
    model = joblib.load(model_path)  
    return model

def _feaprocess(data):#æ•°æ®å¤„ç†
    # test_data, _ = aaindex1(data)
    # test_data_df = pd.DataFrame(test_data)
    test_data_aaindex1, _ = aaindex1(data)
    test_data_ng, _ = ngram(data)
    test_data_ctd, _ = ctd(data)
    test_data_atc, _ = atc(data)
    test_data = np.concatenate(
        (test_data_aaindex1, test_data_ng, test_data_ctd, test_data_atc), axis=1#å››ä¸ªç‰¹å¾å‘é‡æ‹¼æ¥åˆ°ä¸€èµ·
    )
    indices = pd.read_csv("./train_data/indices_df.csv", header=None)
    indices = indices.iloc[:, 0]
    indices = np.array(indices)
    indices = indices.tolist()
    test_data = test_data[:, indices]

    train_data = pd.read_csv("./train_data/reduced_df.csv", header=None)
    train_data = np.array(train_data)
    train_target = pd.read_csv("./train_data/isAMP.csv", header=None)
    train_target = train_target.iloc[:, 0]
    train_target = np.array(train_target)

    return test_data, train_data, train_target
# st.set_page_config(page_title="æŠ—èŒè‚½é¢„æµ‹")
st.set_page_config(
    page_title="æŠ—èŒè‚½é¢„æµ‹æ¨¡å‹",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "About": "# åŸºäºè®¡ç®—æ–¹æ³•çš„æŠ—èŒè‚½é¢„æµ‹æ¨¡å‹",
    },
)
def add_bg_from_local(image_file):#èƒŒæ™¯
    with open(image_file, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read())
    st.markdown(
    f"""
    <style>
    .stApp {{
        background-image: url(data:image/{"png"};base64,{encoded_string.decode()});
        background-size: cover
    }}
    </style>
    """,
    unsafe_allow_html=True
    )
add_bg_from_local("DNA.jpg")
colindex = 0

# st.balloons()
# st.title("æŠ—èŒè‚½ï¼ˆAMPï¼‰é¢„æµ‹")
# st.markdown("### 1.æ•°æ®é¢„å¤„ç†")
centered_text = "<center><h1 >ğŸ§¬ åŸºäºè®¡ç®—æ–¹æ³•çš„æŠ—èŒè‚½é¢„æµ‹æ¨¡å‹</h1></center>" 
st.markdown(centered_text, unsafe_allow_html=True)
footer_html = """
<style>
    .footer {
        position: fixed;
        left: 0;
        bottom: 0;
        width: 100%;
        background-color: #f8f9fa;
        color: black;
        text-align: center;
        padding: 10px;
    }
</style>
<div class="footer">
    <p>åŸºäºè®¡ç®—æ–¹æ³•çš„æŠ—èŒè‚½é¢„æµ‹æ¨¡å‹ &copy;  å­¦æœ¯æœºæ„ï¼šå¾å·åŒ»ç§‘å¤§å­¦</p>
</div>
"""
st.markdown(footer_html, unsafe_allow_html=True)

tab1, tab2, tab3,tab4,tab5,tab6,tab7,tab8 = st.tabs(["æ¨¡å‹ç®€ä»‹", "ä¸»è¦åŠŸèƒ½", "ä¸‹è½½","ä½¿ç”¨è¯´æ˜","ç•™è¨€","å…³äºæˆ‘ä»¬","è”ç³»æ–¹å¼","é“¾æ¥"])
with tab2:
    col1, col2,= st.columns([1,2])
    with col1:
        st.subheader("ğŸ“¤ä¸Šä¼ è‚½åºåˆ—æ•°æ®æˆ–è¾“å…¥è‚½åºåˆ—")
        test_file = st.file_uploader(
            "ä¸Šä¼ éœ€è¦é¢„æµ‹çš„è‚½æ•°æ®(**CSV**æ ¼å¼ï¼Œæ¯è¡Œä¸ºä¸€æ¡è‚½åºåˆ—)", type=["csv"]
        )
        if test_file is not None:
            colindex = 1
            testdata = pd.read_csv(test_file, header=0)
            test_data = testdata.iloc[:, 0] 
            test_data = np.array(test_data)
            test_data = test_data.tolist()
            st.write("è‚½åºåˆ—æ•°æ®é¢„è§ˆï¼Œæ–‡ä»¶ä¸­å…±æœ‰" + str(len(test_data)) + "æ¡è‚½åºåˆ—")
            # å±•ç¤ºtestdataç¬¬ä¸€åˆ—çš„æ•°æ®é¢„è§ˆ
            st.write(testdata.iloc[:, 0])
        else:
            input_text = st.text_input("æˆ–è€…æ‰‹åŠ¨è¾“å…¥è‚½åºåˆ—ï¼ˆå¤šä¸ªè‚½åºåˆ—è¯·ç”¨é€—å·åˆ†éš”ï¼‰")
            colindex = 1
            if input_text:
                test_data = input_text.split(',')
                st.write("è‚½åºåˆ—æ•°æ®é¢„è§ˆ")
                st.write(test_data)
        with col2:
            if colindex == 1:
                st.subheader("ğŸ› ï¸ é€‰æ‹©é¢„æµ‹æ¨¡å‹")
                model = st.radio(
                    label="é€‰æ‹©ä¸€ç§é¢„æµ‹æ¨¡å‹",
                    options=(
                        "éšæœºæ£®æ— RF",
                        "æ¢¯åº¦æå‡ Gradient Boosting Classifier",
                    ),
                    index=0,
                )
                if model == "éšæœºæ£®æ— RF":
                    st.write(
                        "- **æ–¹æ³•è¯´æ˜**ï¼šéšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ„å»ºå¤šä¸ªå†³ç­–æ ‘æ¥é¢„æµ‹ç›®æ ‡å˜é‡ã€‚æ¯ä¸ªå†³ç­–æ ‘éƒ½ä½¿ç”¨ä¸åŒçš„ç‰¹å¾å’Œéšæœºé‡‡æ ·ï¼Œç„¶åé€šè¿‡æŠ•ç¥¨æˆ–å¹³å‡æ¥é¢„æµ‹ç›®æ ‡å˜é‡ã€‚"
                    )
                elif model == "æ¢¯åº¦æå‡ Gradient Boosting Classifier":
                    st.write(
                        "- **æ–¹æ³•è¯´æ˜**ï¼šæ¢¯åº¦æå‡åˆ†ç±»å™¨ï¼ˆGradient Boosting Classifierï¼‰æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºæ¢¯åº¦æå‡ï¼ˆGradient Boostingï¼‰æ¡†æ¶ï¼Œä¸»è¦ç”¨äºè§£å†³åˆ†ç±»é—®é¢˜ã€‚"
                    )
                # æŒ‰é’®ï¼Œå¼€å§‹é¢„æµ‹
                if st.button("å¼€å§‹é¢„æµ‹"):
                    with st.spinner("é¢„æµ‹ä¸­..."):# æ˜¾ç¤ºæ—‹è½¬åŠ è½½å™¨
                        test_data_df, train_data, train_target = _feaprocess(test_data)  
                        if model == "éšæœºæ£®æ— RF":  
                            rf_model = load_rf_model()  
                            pred = rf_model.predict_proba(test_data_df)[:, 1]  
                        elif model == "æ¢¯åº¦æå‡ Gradient Boosting Classifier":  
                            gb_model = load_gb_model()  
                            pred = gb_model.predict_proba(test_data_df)[:, 1]  
                             
                        colindex = 2

            if colindex == 2:  
                st.subheader("ğŸ“ƒ é¢„æµ‹ç»“æœ")  
                pred_proba = pd.DataFrame(pred, columns=["æŠ—èŒè‚½æ¦‚ç‡"])  
                result = pd.concat([pd.DataFrame(test_data, columns=["è‚½åºåˆ—"]), pred_proba], axis=1)  
  
                # åº”ç”¨æ ·å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰  
                # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥é€šè¿‡styleæ”¹å˜æ¦‚ç‡çš„é¢œè‰²ï¼Œä½†å¯ä»¥æ ‡è®°é«˜æ¦‚ç‡çš„æ¡ç›®  
                high_prob_threshold = 0.7  # å‡è®¾æˆ‘ä»¬è®¤ä¸ºæ¦‚ç‡å¤§äº0.7ä¸ºé«˜æ¦‚ç‡  
                 
  
                result.style.apply(lambda x: ['background-color: ' + x['é«˜äº®'] for x in x.to_dict().values()], axis=1)  
  
                # æ±‡æ€»ç»Ÿè®¡  
                high_prob_count = (result['æŠ—èŒè‚½æ¦‚ç‡'] > high_prob_threshold).sum()  
                st.write(f"é«˜æ¦‚ç‡ï¼ˆ>{high_prob_threshold}ï¼‰æ˜¯æŠ—èŒè‚½ï¼ˆAMPï¼‰çš„åºåˆ—å…±{high_prob_count}æ¡ã€‚")  
                #st.write(f"é¢„æµ‹ç»“æœå·²æ˜¾ç¤ºï¼Œæ¯è¡Œè‚½åºåˆ—å¯¹åº”çš„'æŠ—èŒè‚½æ¦‚ç‡'åˆ—æ˜¾ç¤ºäº†å…¶æ˜¯æŠ—èŒè‚½çš„æ¦‚ç‡ã€‚")  
                st.write(result)  # è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯å¸¦æœ‰æ ·å¼çš„ DataFrame  
  
                # é‡æ–°é¢„æµ‹æŒ‰é’®  
                if st.button("é‡æ–°é¢„æµ‹"):  
                    st.experimental_rerun()

st.divider() 
with tab1:
    st.subheader("ğŸ”¬ å…³äºæ¨¡å‹")
    st.markdown("åŸºäºè®¡ç®—æ–¹æ³•çš„æŠ—èŒè‚½é¢„æµ‹æ¨¡å‹")
    st.markdown("---")  
    st.subheader("âœ‚ï¸ ç‰¹å¾å¤„ç†æ–¹æ³•")
    st.markdown(
        "æœ¬æ¨¡å‹ä½¿ç”¨**aaindex1ã€ngramã€ctdã€atc**å››ç§ç‰¹å¾è¡¨ç¤ºæ–¹æ³•å°†è›‹ç™½è´¨åºåˆ—å¤„ç†æˆè®¡ç®—æœºå¯è¯†åˆ«çš„å½¢å¼ï¼Œå¹¶æŒ‰ç…§ç‰¹å¾é‡è¦æ€§æ’åºå¹¶é€‰å–å‰50%çš„ç‰¹å¾ä½œä¸ºæœ€ç»ˆç‰¹å¾å‘é‡ï¼Œä»¥åŠ å¿«æ¨¡å‹é¢„æµ‹é€Ÿåº¦"
    )
    st.subheader("")
    st.subheader("ğŸ“ˆ é¢„æµ‹æ¨¡å‹")
    st.markdown(
        "ä½¿ç”¨è€…å¯è‡ªç”±é€‰ç”¨æˆ‘ä»¬å·²ç»è®­ç»ƒå¥½çš„**éšæœºæ£®æ—ç®—æ³•RF**æˆ–**æ¢¯åº¦æå‡ç®—æ³•GBDT**ä½œä¸ºé¢„æµ‹æŠ—èŒè‚½çš„æ¨¡å‹ï¼Œç»è¿‡æˆ‘ä»¬å®éªŒéªŒè¯ï¼Œè¿™ä¸¤ç§ç®—æ³•éƒ½å…·æœ‰è¾ƒé«˜çš„ç²¾åº¦ï¼ˆACCã€F1 scoreã€ROC_AUC)å’Œé©¬ä¿®ç›¸å…³ç³»æ•°(MCC)"
    )
with tab3:
    st.subheader('ğŸ“¥ä¸‹è½½æ¨¡å‹æ•°æ®')
    data = {  
    'Name': ['Alice', 'Bob', 'Charlie'],  
    'Age': [25, 30, 35],  
    'City': ['New York', 'Los Angeles', 'Chicago']  
    }
    df = pd.DataFrame(data)  
    st.download_button(  
    label="ä¸‹è½½è®­ç»ƒé›†æ•°æ®",  
    data=df.to_csv(index=False).encode('utf-8'),  
    file_name="train_data.csv",  
    mime="./train_data/reduced_df.csv"
    )
    st.download_button(  
    label="ä¸‹è½½è®­ç»ƒé›†æ•°æ ‡ç­¾",  
    data=df.to_csv(index=False).encode('utf-8'),  
    file_name="train_target.csv",  
    mime="./train_data/isAMP.csv"
    )
    st.download_button(  
    label="ä¸‹è½½ç‰¹å¾é€‰å–æ‰€ç”¨æ•°æ®",  
    data=df.to_csv(index=False).encode('utf-8'),  
    file_name="indices_df.csv",  
    mime="./train_data/indices_df.csv"
    )
with tab4:
    st.subheader('ğŸ“šä½¿ç”¨è¯´æ˜')
    st.markdown('1.ä¸Šä¼ æ•°æ®ï¼Œæ­¤å¤„éœ€è¦ä¸Šä¼ csvæ ¼å¼çš„æ•°æ®ã€‚ç‚¹å‡»â€œBrowse filesâ€å³å¯ä»æ–‡ä»¶å¤¹ä¸­é€‰æ‹©éœ€è¦ä¸Šä¼ çš„è›‹ç™½è´¨æ•°æ®ï¼Œæ•°æ®éœ€åŒ…å«æŠ—èŒè‚½åºåˆ—ä¸Šä¼ æˆåŠŸåå¯å¯¹æ•°æ®è¿›è¡Œé¢„è§ˆã€‚ç¬¬ä¸€åˆ—ä¸ºè›‹ç™½è´¨çš„ç¼–å·ï¼Œç¬¬äºŒåˆ—ä¸ºè›‹ç™½è´¨çš„åºåˆ—å±•ç¤ºï¼Œä¸Šä¸‹æ»šåŠ¨å¯æŸ¥çœ‹å®Œæ•´é¢„è§ˆã€‚æˆ–è€…æ‰‹åŠ¨è¾“å…¥è›‹ç™½è´¨åºåˆ—ä¿¡æ¯ã€‚')
    st.divider() 
    st.markdown('2.é€‰æ‹©é¢„æµ‹ç®—æ³•ï¼Œå¯ä»¥é€‰æ‹©æˆ‘ä»¬å¾®è°ƒå¥½çš„éšæœºæ£®æ—æˆ–gbdtç®—æ³•ä½œä¸ºé¢„æµ‹ç®—æ³•ã€‚æ¯ç§ç®—æ³•é€‰æ‹©ä¸‹éƒ½æœ‰å¯¹è¯¥ç®—æ³•çš„ç®€è¦è¯´æ˜ï¼Œç ”ç©¶è€…å¯è‡ªè¡Œé€‰ç”¨éšæœºæ£®æ—ç®—æ³•æˆ–gbdtç®—æ³•ï¼Œè¿™ä¸¤ç§ç®—æ³•éƒ½ç»è¿‡å®éªŒéªŒè¯ï¼Œæ€§èƒ½é«˜ï¼Œé¢„æµ‹èƒ½åŠ›å¼ºã€‚')
    st.divider() 
    st.markdown('3.è¿è¡Œç»“æœï¼Œç»è¿‡è¿ç®—çš„ç»“æœä¼šæ˜¾ç¤ºåœ¨ç½‘é¡µä¸Šï¼Œç ”ç©¶è€…å¯è‡ªä¸»è¿›è¡Œä¸‹è½½æˆ–æŸ¥çœ‹æ“ä½œã€‚æœ¬åº”ç”¨å°†é¢„æµ‹ä¸ºæŠ—èŒè‚½çš„åºåˆ—èµ‹äºˆæ ‡ç­¾â€œæ˜¯â€ï¼Œå°†éæŠ—èŒè‚½çš„åºåˆ—èµ‹äºˆæ ‡ç­¾â€œå¦â€ï¼Œç ”ç©¶è€…ä¹Ÿå¯æ ¹æ®éœ€è¦è‡ªè¡Œæ›¿æ¢ã€‚è¡¨æ ¼ä¸Šæ–¹ä¼šæ ‡å‡ºæœ¬æ¬¡é¢„æµ‹ä¸­æŠ—èŒè‚½å’ŒéæŠ—èŒè‚½çš„æ•°é‡å’Œå æ¯”ã€‚å¦‚å¯¹é¢„æµ‹çš„ç»“æœä¸æ»¡æ„ï¼Œå¯ç‚¹å‡»â€œé‡æ–°é¢„æµ‹â€è¿”å›ä¸Šä¸€æ­¥é‡æ–°è¿è¡Œæ¨¡å‹ã€‚')
with tab5:
    MESSAGES_FILE = 'messages.json'  
  
# åŠ è½½ç•™è¨€å‡½æ•°  
    def load_messages():  
        if os.path.exists(MESSAGES_FILE):  
            with open(MESSAGES_FILE, 'r') as file:  
               messages = json.load(file)  
        else:  
            messages = []  
        return messages  
  
# ä¿å­˜ç•™è¨€å‡½æ•°  
    def save_messages(messages):  
        with open(MESSAGES_FILE, 'w') as file:  
            json.dump(messages, file, indent=4)  
  
# æ·»åŠ ç•™è¨€å‡½æ•°  
    def add_message(user, message, messages):  
        messages.append({"user": user, "message": message})  
        save_messages(messages)  # æäº¤åç«‹å³ä¿å­˜  
  
# æ˜¾ç¤ºç•™è¨€å‡½æ•°  
    def display_messages(messages):  
       for msg in messages:  
            st.write(f"{msg['user']}: {msg['message']}")  
  
# Streamlitåº”ç”¨ä¸»ä½“  
    with st.container():  
        st.subheader("ğŸ“ç•™è¨€æ¿")  
        messages = load_messages()  # åŠ è½½ç•™è¨€  
  
    # ç”¨æˆ·è¾“å…¥ç•™è¨€  
        user = st.text_input("è¯·è¾“å…¥ä½ çš„åå­—ï¼š")  
        message = st.text_area("è¯·è¾“å…¥ä½ çš„ç•™è¨€ï¼š")  
        if st.button("æäº¤ç•™è¨€"):  
            if user and message:  
                add_message(user, message, messages)  # æ·»åŠ å¹¶ä¿å­˜ç•™è¨€  
                st.success("ç•™è¨€å·²æäº¤ï¼")  
            else:  
                st.warning("è¯·å¡«å†™å®Œæ•´ä¿¡æ¯ï¼")  
  
    # æ˜¾ç¤ºæ‰€æœ‰ç•™è¨€  
        st.subheader("ç•™è¨€åˆ—è¡¨ï¼š")  
        display_messages(messages)  # æ˜¾ç¤ºåŠ è½½çš„ç•™è¨€
with tab6:
    st.subheader("â˜ï¸ è”ç³»æˆ‘ä»¬")
    st.markdown("å­¦æœ¯æœºæ„:å¾å·åŒ»ç§‘å¤§å­¦")




